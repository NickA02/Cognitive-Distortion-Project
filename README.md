# Are LLMs Cognitive Distortion Detectors?

This repository contains code for the evaluation of Llama LLM models in detecting cognitive distortions through n-shot prompting. The focus is on assessing whether large language models (LLMs), like Llama, can accurately identify and analyze cognitive distortions in text.

The paper titled **"Are LLMs Cognitive Distortion Detectors?"** accompanies this repository and presents detailed insights into the study. The repository currently supports analysis for 0-shot and 1-shot prompt evaluations, with plans for expanding to more complex n-shot prompts in the future.

## Table of Contents (WIP for when project is more robust)

- [Overview](#overview)

## Overview

Cognitive distortions are irrational thought patterns that can lead to negative thinking and emotions. This repository explores whether LLMs, such as Llama, can be effective tools for identifying these distortions in written text. 

### Key Features:
- **0-shot and 1-shot Evaluation**: The current version supports evaluating the model's performance in identifying cognitive distortions in 0-shot and 1-shot settings.
- **Prompt Design**: The code uses prompt engineering techniques to assess the model's responses and capability to detect specific cognitive distortions.
- **LLM Model Analysis**: Evaluations are performed using the Llama family of large language models.
